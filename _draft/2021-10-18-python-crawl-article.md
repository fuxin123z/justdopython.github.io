---
layout: post
category: python
title: 如何像搜索引擎一样，精准抓取网页内容
tagline: by 李晓飞
tags:
  - Scrapy
  - 爬虫
---
![封面](http://www.justdopython.com/assets/images/2021/10/crawl-page/00.jpg)
爬虫程序想必大家都很熟悉了，随便写一个就可以获取网页上的信息，甚至可以通过请求自动生成 Python 脚本。

最近我遇到一个爬虫项目，需要爬取网上的文章。感觉没有什么特别的，但问题是没有限定爬取范围，意味着没有明确的页面的结构。

对于一个页面来说，除了核心文章内容外，还有头部，尾部，左右列表栏等等。有的页面框架用 div 布局，有的用 table，即使都用 div，不太的网站风格和布局也不同。

但问题必须解决，我想，既然搜索引擎抓取到各种网页的核心内容，我们也应该可以搞定，拎起 Python， 说干就干！

## 各种尝试

如何解决呢？

### 生成PDF

开始想了一个取巧的方法，就是利用工具（[wkhtmltopdf](https://wkhtmltopdf.org/ 'wkhtmltopdf')）将目标网页生成 PDF 文件。

好处是不必关心页面的具体形式，就像给页面拍了一张照片，文章结构是完整的。

虽然 PDF 是可以再源码级检索的，但是，生成 PDF 有诸多缺点：

成长太高、耗费计算资源多、效率低、出错率高，还有体积太大。

几万条数据已经两百多G，如果数据量上来光存储就是很大的问题。

得找其他方法。

### 提取文章内容

最简单的方法就是，通过 [xpath](https://www.w3school.com.cn/xpath/xpath_syntax.asp 'xpath') 提取页面上的所有文字。

但是内容将失去结构，可读性差，更要命的是，网页上有很多无关内容，也会被提取下来。

为了保证有一定的结构，还要识别到核心内容，就只能识别并提取文章部分的结构了。

像搜索引擎学习，就是想办法识别页面的核心内容。

通常情况下，页面上的文章内容比较集中，如果能找到集中部分的结构就可以了。

于是编写了一段代码，我是用 [Scrapy](https://scrapy.org/ 'Scrapy') 作为爬虫框架的，这里只截取了其中提取文章部分的代码 ：

```python
divs = response.xpath("body//div")
sel = None
maxvalue = 0
for d in divs:
  ds = len(d.xpath(".//div"))
  ps = len(d.xpath(".//p"))
  value = ps - ds
  if value > maxvalue:
    sel = {
      "node": d,
      "value": value
    }
    maxvalue = value

print("".join(sel['node'].getall()))
```

- `response` 是页面的一个相应，其中包含了页面的所有内容，可以通过 `xpath` 提取想要的部分
- `"body//div"` 的意思是提取所以 `body` 标签下的 `div` 子标签，注意： `//` 操作是递归的
- 遍历所有提取到的标签，计算其中包含的 `div` 数量，和 `p` 数量
- `p` 数量 和 `div` 数量的差值作为这个元素的权值，意思是如果这个元素里包含了大量的 `p` 时，就认为这里是文章主体
- 通过比较权值，选择出权值最大的元素，这便是文章主体
- 得到文章主体之后，提取这个元素的内容，相当于 [jQuery](jquery.com "jQuery") 的 `outerHtml`

简单明了，测试了几个页面确实挺好。

不过大量提取时发现，很多页面提取不到数据。仔细查看发现，有两种情况。

1. 有的文章内容被放在了 `<article>` 标签里了，所以没有获取到

2. 有的文章每个 `<p>` 外面都包裹了一个 `<div>`，所以 `p` 的数量 和 `div` 的抵消了

再调整了一下策略，不再区分 `div`，查看所有的元素。

另外优先选择更多的 `p`，在其基础上再看更少的 `div`。调整后的代码如下：

```python
divs = response.xpath("body//*")
sels = []
maxvalue = 0
for d in divs:
  ds = len(d.xpath(".//div"))
  ps = len(d.xpath(".//p"))
  if ps >= maxvalue:
    sel = {
      "node": d,
      "ps": ps,
      "ds": ds
    }
    maxvalue = ps
    sels.append(sel)

sels.sort(lambda x: x.ds)

sel = sels[0]

print("".join(sel['node'].getall()))
```

- 方法主体里，先挑选出 `p` 数量比较大的节点，注意 `if` 判断条件中 换成了 `>=` 号，作用时筛选出同样具有 `p` 数量的结点
- 经过筛选之后，按照 `div` 数量排序，然后选取 `div` 数量最少的

经过这样修改之后，确实在一定程度上弥补了前面的问题，但是引入了一个更麻烦的问题。

就是找到的文章主体不稳定，特别容易受到其他部分有些 `p` 的影响。

## 选择最优

既然直接计算不太合适，需要重新设计一个算法。

我发现，文字集中的地方是往往是文章主体，而前面的方法中，没有考虑到这一点，只是机械地找出了最大地 `p`。

还有一点，网页结构是个颗 [DOM 树](https://baike.baidu.com/item/DOM%20Tree/6067246 'DOM 树')
![DOM 树](http://www.justdopython.com/assets/images/2021/10/crawl-page/01.png)

那么越靠近 `p` 标签的地方应该越可能是文章主体，也就是说，计算是越靠近 `p` 的节点权值应该越大，而远离 `p` 的结点及时拥有很多 `p` 但是权值也应该小一点。

经过试错，最终代码如下：

```python
def find(node, sel):
    value = 0
    for n in node.xpath("*"):
        if n.xpath("local-name()").get() == "p":
            t = "".join([s.strip() for s in (n.xpath('text()').getall() + n.xpath("*/text()").getall())])
            value += len(t)
        else:
            value += find(n, a)*0.5
    if value > sel["value"]:
        sel["node"] = node
        sel["value"] = value
    return value

sel = {
    'value': 0,
    'node': None
}
find(response.xpath("body"), sel)
```

- 定义了一个 `find` 函数，这是为了方便做递归，第一次调用的参数是 `body` 标签，和前面一样
- 进入方法里，只找出该节点的直接孩子们，然后遍历这些孩子
- 判断如果孩子是 `p` 节点，提取出其中的所有文字，包括子节点的，然后将文字的长度作为权值
- 提取文字的地方比较绕，先取出直接的文本，和间接文本，合成 `list`，对每部分文本做了去除前后空字符，最后合并为一个字符串，得到了所包含的文本
- 如果孩子节点不是 `p`，就递归调用 `find` 方法，而 `find` 方法返回的是 指定节点所包含的文本长度
- 在获取子节点的长度时，做了缩减处理，用以体现距离越远，权值越低的规则
- 最终通过 引用传递的 `sel` 参数，记录权值最高的节点

通过这样改造之后，效果特别好。

为什么呢？其实利用了密度原理，就是说越靠近中心的地方，密度越高，远离中心的地方密度成倍的降低，这样就能筛选出密度中心了。

50% 的坡度比率是如何得到的呢？

其实是通过实验确定的，刚开始时我设置为 90%，但结果时 `body` 节点总是最优的，因为 `body` 里包含了所有的文字内容。

反复实验后，确定 50% 是比较好的值，如果在你的应用中不合适，可以做调整。

## 总结

描述了我如何选取文章主体的方法后，后没有发现其实很是很简单的方法。而这次解决问题的经历，让我感受到了数学的魅力。

一直以来我认为只要了解常规处理问题的方式就足以应对日常编程了，可以当遇到不确定性问题，没有办法抽取出简单模型的问题时，常规思维显然不行。

所以平时我们应该多看一些数学性强的，解决不确定性问题的方法，以便提高我们的编程适应能力，扩展我们的技能范围。

期望这篇短文能对你有所启发，欢迎在留言区交流讨论，比心！
